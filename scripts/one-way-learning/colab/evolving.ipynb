{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwUJ-S55vQB2",
        "outputId": "c87647f9-de89-4520-8c73-59708ca4e38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: farama-notifications, smmap, setproctitle, sentry-sdk, gymnasium, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 farama-notifications-0.0.4 gitdb-4.0.11 gymnasium-0.29.1 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import pygame\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import wandb"
      ],
      "metadata": {
        "id": "zNJOSW-qvRG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c98Y0CPrvI4S"
      },
      "outputs": [],
      "source": [
        "class CustomEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CustomEnv, self).__init__()\n",
        "\n",
        "        pygame.init()\n",
        "        self.WIDTH, self.HEIGHT = 320, 240\n",
        "        self.screen = pygame.display.set_mode((self.WIDTH, self.HEIGHT))\n",
        "\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.HEIGHT, self.WIDTH, 3), dtype=np.uint8)\n",
        "\n",
        "        self.organism_size = 20\n",
        "        self.organism_direction = 0  # Başlangıç yönü\n",
        "        self.organism_speed = 5\n",
        "        self.organism_radius = self.organism_size // 2\n",
        "        self.organism_position = [self.WIDTH // 2, self.HEIGHT // 2]\n",
        "\n",
        "        self.goal_size = 20\n",
        "        self.goal_radius = self.organism_radius\n",
        "        self.goal_position = [100, 100]\n",
        "\n",
        "        self.score = 0\n",
        "        self.learning_rate = 0.1\n",
        "        self.discount_factor = 0.9\n",
        "        self.epsilon = 0.3\n",
        "\n",
        "        self.state_space_size = (self.WIDTH // self.organism_size, self.HEIGHT // self.organism_size)\n",
        "        self.q_table = np.zeros((self.state_space_size[0], self.state_space_size[1], self.action_space.n))\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = -0.1\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        angle_change = math.pi / 8\n",
        "        x, y = self.organism_position\n",
        "\n",
        "        if action == 0:\n",
        "            y -= self.organism_speed\n",
        "        elif action == 1:\n",
        "            y += self.organism_speed\n",
        "        elif action == 2:\n",
        "            x -= self.organism_speed\n",
        "            self.organism_direction -= angle_change\n",
        "        elif action == 3:\n",
        "            x += self.organism_speed\n",
        "            self.organism_direction += angle_change\n",
        "\n",
        "        x = max(self.organism_radius, min(x, self.WIDTH - self.organism_radius))\n",
        "        y = max(self.organism_radius, min(y, self.HEIGHT - self.organism_radius))\n",
        "\n",
        "        self.organism_position = [x, y]\n",
        "\n",
        "        distance_to_goal = math.sqrt((x - self.goal_position[0])**2 + (y - self.goal_position[1])**2)\n",
        "        if distance_to_goal < self.goal_radius:\n",
        "            reward += 10\n",
        "            done = True\n",
        "            self.goal_position = [random.randint(0, (self.WIDTH - self.goal_size) // self.organism_size) * self.organism_size,\n",
        "                                  random.randint(0, (self.HEIGHT - self.goal_size) // self.organism_size) * self.organism_size]\n",
        "\n",
        "        observation = np.array(pygame.surfarray.array3d(self.screen))\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.organism_position = [self.WIDTH // 2, self.HEIGHT // 2]\n",
        "        self.score = 0\n",
        "        observation = np.array(pygame.surfarray.array3d(self.screen))\n",
        "        return observation\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        self.screen.fill((255, 255, 255))\n",
        "        pygame.draw.circle(self.screen, (255, 0, 0), self.organism_position, self.organism_radius)\n",
        "        pygame.draw.circle(self.screen, (0, 255, 0), self.goal_position, self.goal_radius)\n",
        "        pygame.display.update()\n",
        "\n",
        "    def close(self):\n",
        "        pygame.quit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_space, action_space):\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.memory = deque(maxlen=20000)\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self.build_model()\n",
        "\n",
        "        wandb.config = {\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"epsilon_decay\": self.epsilon_decay,\n",
        "            \"gamma\": self.gamma,\n",
        "            \"epsilon_initial\": self.epsilon,\n",
        "            \"epsilon_min\": self.epsilon_min\n",
        "        }\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=(self.state_space[0], self.state_space[1], self.state_space[2])))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(48, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(self.action_space, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_space)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return 0  # Yeterli deneyim yoksa, eğitim yapma\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Tüm mini-batch için hedefleri hesapla\n",
        "        states = np.array([i[0] for i in minibatch])\n",
        "        next_states = np.array([i[3] for i in minibatch])\n",
        "        targets = np.array([self.calculate_target(state, action, reward, next_state, done)\n",
        "                            for (state, action, reward, next_state, done) in minibatch])\n",
        "\n",
        "        # Tüm mini-batch üzerinde modeli eğit\n",
        "        history = self.model.fit(states, np.vstack(targets), epochs=1, verbose=0, batch_size=batch_size)\n",
        "\n",
        "        training_duration = time.time() - start_time\n",
        "        average_loss = np.mean(history.history['loss'])\n",
        "\n",
        "        wandb.log({\n",
        "            \"average_loss\": average_loss,\n",
        "            \"epsilon\": self.epsilon,\n",
        "            \"training_duration\": training_duration\n",
        "        })\n",
        "\n",
        "        self.update_epsilon()\n",
        "        return average_loss\n",
        "\n",
        "\n",
        "    def calculate_target(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
        "        target_f = self.model.predict(state)\n",
        "        target_f[0][action] = target\n",
        "        return target_f"
      ],
      "metadata": {
        "id": "qEfWcMXSzCAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "wandb.init(project='evolving', entity='fth123bng')\n",
        "\n",
        "env = CustomEnv()  # CustomEnv sınıfınızın adını kullanın\n",
        "state_size = env.observation_space.shape\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# DQN ajanınızı başlatın\n",
        "dqn_agent = DQN(state_size, action_size)\n",
        "\n",
        "# Eğitim parametreleri\n",
        "episodes = 100\n",
        "batch_size = 32\n",
        "\n",
        "model_save_path = 'dqn_models'\n",
        "if not os.path.exists(model_save_path):\n",
        "    os.makedirs(model_save_path)\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size[0], state_size[1], state_size[2]])\n",
        "    total_reward = 0\n",
        "\n",
        "    for time in range(500):\n",
        "        action = dqn_agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size[0], state_size[1], state_size[2]])\n",
        "        # Deneyimi hafızaya kaydet\n",
        "        dqn_agent.remember(state, action, reward, next_state, done)\n",
        "        total_reward += reward\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            print(f\"Episode: {e}/{episodes}, Score: {time}, Total Reward: {total_reward}\")\n",
        "            wandb.log({\"episode\": e, \"score\": time, \"total_reward\": total_reward})\n",
        "            break\n",
        "\n",
        "        if len(dqn_agent.memory) > batch_size:\n",
        "            dqn_agent.replay(batch_size)\n",
        "\n",
        "    if dqn_agent.epsilon > dqn_agent.epsilon_min:\n",
        "        dqn_agent.epsilon *= dqn_agent.epsilon_decay\n",
        "\n",
        "    # Her 10 bölümde bir modeli kaydet\n",
        "    if e % 10 == 0:\n",
        "        model_filename = os.path.join(model_save_path, f'dqn_model_{e}.h5')\n",
        "        dqn_agent.model.save(model_filename)\n",
        "        print(f\"Model saved: {model_filename}\")\n",
        "\n",
        "\n",
        "dqn_agent.model.save(os.path.join(model_save_path, 'dqn_model_final.h5'))\n",
        "print(\"Final model saved\")\n",
        "wandb.save(os.path.join(model_save_path, 'dqn_model_final.h5'))\n"
      ],
      "metadata": {
        "id": "QXhwX-Sp07sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}