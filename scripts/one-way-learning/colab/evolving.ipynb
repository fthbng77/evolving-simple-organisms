{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "lwUJ-S55vQB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import pygame\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "zNJOSW-qvRG_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c98Y0CPrvI4S"
      },
      "outputs": [],
      "source": [
        "class CustomEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CustomEnv, self).__init__()\n",
        "\n",
        "        pygame.init()\n",
        "        self.WIDTH, self.HEIGHT = 320, 240\n",
        "        self.screen = pygame.display.set_mode((self.WIDTH, self.HEIGHT))\n",
        "\n",
        "        self.action_space = spaces.Discrete(4)  # 4 hareket (yukarı, aşağı, sağ, sol)\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.HEIGHT, self.WIDTH, 3), dtype=np.uint8)\n",
        "\n",
        "        self.organism_size = 20\n",
        "        self.organism_direction = 0  # Başlangıç yönü\n",
        "        self.organism_speed = 5\n",
        "        self.organism_radius = self.organism_size // 2\n",
        "        self.organism_position = [self.WIDTH // 2, self.HEIGHT // 2]\n",
        "\n",
        "        self.goal_size = 20\n",
        "        self.goal_radius = self.organism_radius\n",
        "        self.goal_position = [100, 100]\n",
        "\n",
        "        self.score = 0\n",
        "        self.learning_rate = 0.1\n",
        "        self.discount_factor = 0.9\n",
        "        self.epsilon = 0.3\n",
        "\n",
        "        self.state_space_size = (self.WIDTH // self.organism_size, self.HEIGHT // self.organism_size)\n",
        "        self.q_table = np.zeros((self.state_space_size[0], self.state_space_size[1], self.action_space.n))\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = -0.1\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        angle_change = math.pi / 8\n",
        "        x, y = self.organism_position\n",
        "\n",
        "        if action == 0:\n",
        "            y -= self.organism_speed\n",
        "        elif action == 1:\n",
        "            y += self.organism_speed\n",
        "        elif action == 2:\n",
        "            x -= self.organism_speed\n",
        "            self.organism_direction -= angle_change\n",
        "        elif action == 3:\n",
        "            x += self.organism_speed\n",
        "            self.organism_direction += angle_change\n",
        "\n",
        "        x = max(self.organism_radius, min(x, self.WIDTH - self.organism_radius))\n",
        "        y = max(self.organism_radius, min(y, self.HEIGHT - self.organism_radius))\n",
        "\n",
        "        self.organism_position = [x, y]\n",
        "\n",
        "        distance_to_goal = math.sqrt((x - self.goal_position[0])**2 + (y - self.goal_position[1])**2)\n",
        "        if distance_to_goal < self.goal_radius:\n",
        "            reward += 10\n",
        "            done = True\n",
        "            self.goal_position = [random.randint(0, (self.WIDTH - self.goal_size) // self.organism_size) * self.organism_size,\n",
        "                                  random.randint(0, (self.HEIGHT - self.goal_size) // self.organism_size) * self.organism_size]\n",
        "\n",
        "        observation = np.array(pygame.surfarray.array3d(self.screen))\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.organism_position = [self.WIDTH // 2, self.HEIGHT // 2]\n",
        "        self.score = 0\n",
        "        observation = np.array(pygame.surfarray.array3d(self.screen))\n",
        "        return observation\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        self.screen.fill((255, 255, 255))\n",
        "        pygame.draw.circle(self.screen, (255, 0, 0), self.organism_position, self.organism_radius)\n",
        "        pygame.draw.circle(self.screen, (0, 255, 0), self.goal_position, self.goal_radius)\n",
        "        pygame.display.update()\n",
        "\n",
        "    def close(self):\n",
        "        pygame.quit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_space, action_space):\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        self.gamma = 0.95    # discount factor\n",
        "        self.epsilon = 1.0   # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=(self.state_space[0], self.state_space[1], self.state_space[2])))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_space, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_space)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "qEfWcMXSzCAp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Önce ortamınızı başlatın\n",
        "env = CustomEnv()  # CustomEnv sınıfınızın adını kullanın\n",
        "state_size = env.observation_space.shape\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# DQN ajanınızı başlatın\n",
        "dqn_agent = DQN(state_size, action_size)\n",
        "\n",
        "# Eğitim parametreleri\n",
        "episodes = 10\n",
        "batch_size = 32\n",
        "\n",
        "for e in range(episodes):\n",
        "    # Ortamı başlatın ve ilk durumu alın\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size[0], state_size[1], state_size[2]])\n",
        "\n",
        "    for time in range(50):\n",
        "        # Ajanın bir eylem seçmesi\n",
        "        action = dqn_agent.act(state)\n",
        "\n",
        "        # Eylemi gerçekleştir ve sonucu al\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size[0], state_size[1], state_size[2]])\n",
        "\n",
        "        # Deneyimi hafızaya kaydet\n",
        "        dqn_agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            print(\"Episode: {}/{}, Score: {}\".format(e, episodes, time))\n",
        "            break\n",
        "\n",
        "        # Hafızada yeterli deneyim biriktiğinde eğitimi başlat\n",
        "        if len(dqn_agent.memory) > batch_size:\n",
        "            dqn_agent.replay(batch_size)\n",
        "\n",
        "# Eğitim tamamlandıktan sonra modeli kaydedebilirsiniz\n",
        "dqn_agent.model.save('dqn_model.h5')\n"
      ],
      "metadata": {
        "id": "QXhwX-Sp07sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}